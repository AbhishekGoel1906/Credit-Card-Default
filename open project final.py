# -*- coding: utf-8 -*-
"""Finclub_Credit_Default_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NsND6sP7kHCdnRN9t3yyTBTFo9NSsp04

# Finclub Summer Project 2 (2025)

## Credit Card Default Prediction

This notebook contains:
- EDA
- Feature Engineering
- Model Training
- Final Predictions
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score, roc_auc_score, fbeta_score
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix

# Load datasets
train_df = pd.read_csv("train_dataset_final1.csv")
val_df = pd.read_csv("validate_dataset_final.csv")

def clean_data(df):

    df = df.drop_duplicates(subset='Customer_ID')
    df['marriage'] = df['marriage'].replace({0: 3, 4: 3})
    median_age = df['age'].median()
    df['age'] = df['age'].apply(lambda x: median_age if (x < 18 or x > 100) else x)
    df = df[df['LIMIT_BAL'] >= 0]


    return df

# Fill missing age
train_df['age'].fillna(train_df['age'].median(), inplace=True)
val_df['age'].fillna(val_df['age'].median(), inplace=True)



# Feature engineering
def engineer_features(df):
    pay_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
    df['credit_utilization_ratio'] = df['AVG_Bill_amt'] / df['LIMIT_BAL']
    df['delinquency_count'] = df[pay_cols].apply(lambda row: sum(row >= 1), axis=1)
    df['on_time_payment_ratio'] = df[pay_cols].apply(lambda row: sum(row == -1) / len(pay_cols), axis=1)
    df['recent_delinquency'] = df['pay_0']
    return df

train_df = engineer_features(train_df)
val_df = engineer_features(val_df)

# Class distribution
sns.countplot(x='next_month_default', data=train_df)
plt.title('Default Rate Distribution')
plt.show()

# Unpivot pay status columns
pay_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
melted = train_df.melt(id_vars='next_month_default', value_vars=pay_cols,
                       var_name='Month', value_name='Pay_Status')

# Boxplot of pay status per month grouped by default
plt.figure(figsize=(10, 6))
sns.boxplot(x='Month', y='Pay_Status', hue='next_month_default', data=melted)
plt.title("Monthly Payment Status vs Default")
plt.ylabel("Payment Status (-2: no usage, -1: paid, 0: partial, 1+: months overdue)")
plt.xlabel("Month")
plt.legend(title="Default Next Month")
plt.show()

# Create utilization bands
train_df['util_band'] = pd.cut(train_df['credit_utilization_ratio'],
                               bins=[-np.inf, 0.3, 0.7, 1.0, np.inf],
                               labels=['Low (<30%)', 'Medium (30â€“70%)', 'High (70â€“100%)', 'Extreme (>100%)'])

# Plot default rate per band
plt.figure(figsize=(8, 5))
sns.barplot(x='util_band', y='next_month_default', data=train_df)
plt.title("Default Rate by Credit Utilization")
plt.ylabel("Default Rate")
plt.xlabel("Utilization Band")
plt.show()

# Calculate repayment consistency as std deviation of past 6 months' payments
pay_amt_cols = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']
train_df['repayment_consistency'] = train_df[pay_amt_cols].std(axis=1)


# Bin customers into quartiles based on repayment consistency
train_df['repayment_consistency_band'] = pd.qcut(train_df['repayment_consistency'], q=4, labels=['Q1 (Stable)', 'Q2', 'Q3', 'Q4 (Most Inconsistent)'])

# Plot default rate per consistency band
plt.figure(figsize=(8, 5))
sns.barplot(x='repayment_consistency_band', y='next_month_default', data=train_df)
plt.title("Default Rate by Repayment Consistency")
plt.ylabel("Default Rate")
plt.xlabel("Repayment Consistency Band")
plt.show()

# Define a function to compute the longest delinquency streak (consecutive pay_m >= 1)
def calc_delinquency_streak(row):
    values = (row >= 1).astype(int)
    max_streak = streak = 0
    for val in values:
        if val == 1:
            streak += 1
            max_streak = max(max_streak, streak)
        else:
            streak = 0
    return max_streak

# Load the validation dataset (if this is for validation)
val_df = pd.read_csv("validate_dataset_final.csv")
val_customers = val_df['Customer_ID']
X_val = val_df.drop(columns=['Customer_ID'])

pay_status_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
X_val['delinquency_streak'] = X_val[pay_status_cols].apply(calc_delinquency_streak, axis=1)


# Apply function to pay status columns
train_df['delinquency_streak'] = train_df[pay_status_cols].apply(calc_delinquency_streak, axis=1)



# Plot delinquency streak vs default
plt.figure(figsize=(8, 5))
sns.barplot(x='delinquency_streak', y='next_month_default', data=train_df)
plt.title("Default Rate by Delinquency Streak Length")
plt.xlabel("Longest Consecutive Months Delinquent")
plt.ylabel("Default Rate")
plt.show()

# Features and target
X = train_df.drop(columns=['Customer_ID', 'next_month_default'])
y = train_df['next_month_default']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Standard pipeline
def make_pipeline(model):
    return Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('model', model)
    ])

models = {
    'Logistic Regression': make_pipeline(LogisticRegression(max_iter=1000, class_weight='balanced')),
    'Decision Tree': make_pipeline(DecisionTreeClassifier(class_weight='balanced')),
    'XGBoost': make_pipeline(XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=4.2)),
    'LightGBM': make_pipeline(LGBMClassifier(class_weight='balanced'))
}

X_train = X_train.drop(columns=['util_band', 'repayment_consistency_band'], errors='ignore')
X_test = X_test.drop(columns=['util_band', 'repayment_consistency_band'], errors='ignore')


results = {}

for name, pipeline in models.items():
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    y_prob = pipeline.predict_proba(X_test)[:, 1]

    results[name] = {
        'F1 Score': f1_score(y_test, y_pred),
        'F2 Score': fbeta_score(y_test, y_pred, beta=2),
        'ROC AUC': roc_auc_score(y_test, y_prob)
    }

results_df = pd.DataFrame(results).T
print("Model Comparison:")
print(results_df)

# Load validation data (same columns as train, except no 'next_month_default')
val_df = pd.read_csv("validate_dataset_final.csv")

# Save Customer_ID for output
val_customers = val_df['Customer_ID']

# Drop non-feature columns
X_val = val_df.drop(columns=['Customer_ID'])

# Recalculate any engineered features
X_val['credit_utilization_ratio'] = X_val['AVG_Bill_amt'] / X_val['LIMIT_BAL']

pay_amt_cols = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']
X_val['repayment_consistency'] = X_val[pay_amt_cols].std(axis=1)

pay_status_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']
X_val['delinquency_count'] = X_val[pay_status_cols].apply(lambda row: sum(row >= 1), axis=1)
X_val['recent_delinquency'] = X_val['pay_0']
X_val['on_time_payment_ratio'] = X_val[pay_status_cols].apply(lambda row: sum(row == -1) / len(row), axis=1)


X_val['delinquency_streak'] = X_val[pay_status_cols].apply(calc_delinquency_streak, axis=1)

# Drop any EDA-only columns (if created earlier)
X_val = X_val.drop(columns=['util_band', 'repayment_consistency_band'], errors='ignore')

# Store training features (excluding target and ID)
drop_cols = ['Customer_ID', 'next_month_default', 'util_band', 'repayment_consistency_band']
X = train_df.drop(columns=drop_cols, errors='ignore')
y = train_df['next_month_default']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Manual preprocessing
imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()

X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

# Step 1: Get the trained pipeline
pipeline = models['LightGBM']  # use your already trained pipeline

# Step 2: Predict probabilities on raw test data
y_prob_test = pipeline.predict_proba(X_test)[:, 1]  # X_test is NOT scaled manually

# Step 3: Sweep thresholds
thresholds = np.arange(0.1, 0.9, 0.01)
f1s = []
f2s = []

for t in thresholds:
    preds = (y_prob_test >= t).astype(int)
    f1s.append(f1_score(y_test, preds))
    f2s.append(fbeta_score(y_test, preds, beta=2))

# Step 4: Find best thresholds
best_f1_thresh = thresholds[np.argmax(f1s)]
best_f2_thresh = thresholds[np.argmax(f2s)]

# Step 5: Plot
plt.figure(figsize=(10, 5))
plt.plot(thresholds, f1s, label='F1 Score', color='blue')
plt.plot(thresholds, f2s, label='F2 Score', color='orange')
plt.axvline(best_f1_thresh, color='blue', linestyle='--', label=f'Best F1: {best_f1_thresh:.2f}')
plt.axvline(best_f2_thresh, color='red', linestyle='--', label=f'Best F2: {best_f2_thresh:.2f}')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Threshold Tuning for LightGBM")
plt.legend()
plt.grid(True)
plt.show()

# Step 6: Print best values
print(f"âœ… Best F1 Threshold: {best_f1_thresh:.2f} with F1 Score = {max(f1s):.4f}")
print(f"âœ… Best F2 Threshold: {best_f2_thresh:.2f} with F2 Score = {max(f2s):.4f}")

from sklearn.metrics import f1_score, fbeta_score, precision_score

# Predict probabilities
y_prob_test = pipeline.predict_proba(X_test)[:, 1]

# Thresholds to test
thresholds = np.arange(0.1, 0.9, 0.01)

# Store scores
f1s = []
f2s = []
precisions = []

# Compute scores at each threshold
for t in thresholds:
    preds = (y_prob_test >= t).astype(int)
    f1s.append(f1_score(y_test, preds))
    f2s.append(fbeta_score(y_test, preds, beta=2))
    precisions.append(precision_score(y_test, preds))

# Convert to NumPy arrays for indexing
f1s = np.array(f1s)
f2s = np.array(f2s)
precisions = np.array(precisions)

# Step 1: Find max F2
max_f2 = f2s.max()

# Step 2: Accept thresholds within 95% of max F2
acceptable = np.where(f2s >= 0.95 * max_f2)[0]

# Step 3: Among them, choose one with highest F1 (or precision)
best_idx = acceptable[np.argmax(f1s[acceptable])]  # or np.argmax(precisions[acceptable])
best_thresh_tradeoff = thresholds[best_idx]

# Print tradeoff result
print(f"ðŸŽ¯ Selected Threshold (F2-prioritized with F1 tradeoff): {best_thresh_tradeoff:.2f}")
print(f"   F2 Score: {f2s[best_idx]:.4f}")
print(f"   F1 Score: {f1s[best_idx]:.4f}")
print(f"   Precision: {precisions[best_idx]:.4f}")

# Load validation dataset
val_df = pd.read_csv("validate_dataset_final.csv")
val_customers = val_df['Customer_ID']

# Drop ID
X_val = val_df.drop(columns=['Customer_ID'])

# Recreate features matching training set
X_val['credit_utilization_ratio'] = X_val['AVG_Bill_amt'] / X_val['LIMIT_BAL']

pay_amt_cols = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']
pay_status_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']

X_val['repayment_consistency'] = X_val[pay_amt_cols].std(axis=1)
X_val['delinquency_count'] = X_val[pay_status_cols].apply(lambda row: sum(row >= 1), axis=1)
X_val['on_time_payment_ratio'] = X_val[pay_status_cols].apply(lambda row: sum(row == -1) / len(row), axis=1)
X_val['recent_delinquency'] = X_val['pay_0']


X_val['delinquency_streak'] = X_val[pay_status_cols].apply(calc_delinquency_streak, axis=1)
final_features = X_train.columns.tolist()

# Align columns to X_train
X_val = X_val[final_features]

# Preprocess with same transformers
X_val_imputed = imputer.transform(X_val)
X_val_scaled = scaler.transform(X_val_imputed)

# Predict probabilities
val_probs = pipeline.predict_proba(X_val_scaled)[:, 1]

# Predict probabilities on validation set
val_probs = pipeline.predict_proba(X_val)[:, 1]

# Choose your threshold (best_f2_thresh or best_thresh_tradeoff)
threshold = best_thresh_tradeoff

# Plot histogram
plt.figure(figsize=(8, 6))
plt.hist(val_probs, bins=50, color='steelblue', edgecolor='black')
plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold:.2f}')
plt.title("Prediction Probabilities on Validation Set")
plt.xlabel("Probability of Default")
plt.ylabel("Number of Customers")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

best_thresh = 0.47
val_preds = (val_probs >= best_thresh).astype(int)
print("Predicted defaulters:", val_preds.sum())

submission_df = pd.DataFrame({
    'Customer': val_customers,
    'next_month_default': val_preds
})

submission_df.to_csv("final_predictions.csv", index=False)

print("final_predictions.csv generated successfully!")
files.download("final_predictions.csv")